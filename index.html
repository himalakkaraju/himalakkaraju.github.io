
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="Hima Lakkaraju" content="">
    <meta name="Hima Lakkaraju" content="">

    <title>Hima Lakkaraju</title>

    <!-- Nunito Sans font -->
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Patrick+Hand+SC" rel="stylesheet">
    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap theme -->
    <link href="css/bootstrap-theme.min.css" rel="stylesheet">
    <!-- font awesome icons -->
    <link href="css/icon.css" rel="stylesheet">
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="css/ie10-viewport-bug-workaround.css" rel="stylesheet">
    <!-- Overriding personal style -->
    <link href="css/style.css" rel="stylesheet">   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> 
  <script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>
    <style>
	  	ul.no-bullets {
  				list-style-type: none;
  				margin: 0;
  				padding: 0;
			       }
    </style>
  </head>

  <body>
    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Hima Lakkaraju</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
          <ul class="nav navbar-nav">
            <li><a href="#">Home</a></li>
            <li><a href="#news">News</a></li>	    
            <li><a href="#research">Research</a></li>
	    <li><a href="#students">Advising</a></li>
            <li><a href="#teaching">Teaching</a></li>
	    <li><a href="https://himalakkaraju.github.io/openpositions.html">Open Positions</a></li>
            <!--<li><a href="#papers">Service</a></li>-->
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container theme-showcase" role="main">
      <div class="row">
	<div class="col-md-3">
	  <h3>Hima Lakkaraju</h3>
	  <img class="img-thumbnail" src="images/hima-2019.jpg" alt="" style="width:70%"/>
	  <br /><br />
	  <h4>Contact</h4>
	<font size="3"><a href="mailto:hlakkaraju@hbs.edu" class="icon fa-envelope"></a> hlakkaraju@hbs.edu<br />
	<font size="3"><a href="mailto:hlakkaraju@seas.harvard.edu" class="icon fa-envelope"></a> hlakkaraju@seas.harvard.edu<br />
	<br>
	<a href="https://www.google.com/maps/place/Morgan+Hall/@42.3667845,-71.1265611,17z/data=!3m1!4b1!4m6!3m5!1s0x89e37760e21ea9e1:0x741d6b281be359d7!8m2!3d42.3667845!4d-71.1239862!16s%2Fg%2F11cly_200s?entry=ttu" class="icon fa-building"></a> Morgan Hall 491<br />
	<a href="https://www.google.com/maps/place/Science+%26+Engineering+Complex/@42.406037,-71.1219121,17z/data=!3m1!4b1!4m6!3m5!1s0x89e377378cf93c53:0xc17eac62367d9b99!8m2!3d42.4060331!4d-71.1170412!16s%2Fg%2F11pkcgblk0?entry=ttu" class="icon fa-building"></a> Science and Engineering Complex 6.220<br />
	<br><a href="https://twitter.com/hima_lakkaraju" class="icon fa-twitter"></a> @hima_lakkaraju
	<br> <a href="https://github.com/AI4LIFE-GROUP" class="icon fa-github"></a> lvhimabindu<br />	  </font>
	</div>
	
	<div class="col-md-9">
	  <br /><br />
	  <h3></h3>
	  <font size="3">

<p>I am an <font color="#cc0000">Assistant Professor at Harvard University</font> with appointments in the Business School and the Department of Computer Science. I am also a Senior Staff Research Scientist (part-time) at Google. Previously, I earned my PhD in Computer Science from Stanford University and have held research and leadership roles at Microsoft Research, IBM Research, Adobe, and Fiddler AI.</p> <blockquote style=" padding: 10px; background-color: #eeeeee;"><font size="3">
        <p>
My research interests lie within the broad area of the <font color="#cc0000">algorithmic foundations and societal implications</font> of safe, trustworthy, and responsible AI. Specifically, I develop <font color="#cc0000">machine learning and optimization techniques</font>, design <font color="#cc0000">evaluation frameworks</font>, and conduct <font color="#cc0000">human-subject studies</font> to improve the trustworthiness of predictive and generative models, including large language models (LLMs). My work spans themes such as safety, interpretability, fairness, privacy, reasoning, AI-assisted decision making, and human–AI collaboration.
</p>

<p>
My work addresses fundamental questions at the intersection of human and algorithmic decision-making, such as:
</p>
<ol>
  <li>How can we build <font color="#cc0000">interpretable</font> and <font color="#cc0000">accurate models</font> to assist and augment human decision-making?</li>
  <li>How do we identify and correct underlying <font color="#cc0000">biases</font> in both human decisions and model predictions?</li>
  <li>How can we ensure that models and their interpretations are robust to <font color="#cc0000">adversarial</font> and <font color="#cc0000">privacy attacks</font>?</li>
  <li>How do we train and evaluate models in the presence of <font color="#cc0000">missing counterfactuals</font> and <font color="#cc0000">unmeasured confounding</font>?</li>
  <li>How do humans engage with AI models, and what factors shape effective <font color="#cc0000">human–AI collaboration</font>?</li>
</ol>

<p>
These questions have far-reaching implications in high-stakes domains such as <font color="#cc0000">health care, policy, law, and business</font>.
</p>

	
	</font>
	
</blockquote>
		  <p> I lead the <font color="#cc0000">AI4LIFE</font> research group at Harvard and I recently co-founded the <a href="https://www.trustworthyml.org/">Trustworthy ML Initiative (TrustML)</a> to help lower entry barriers into trustworthy ML and bring together researchers and practitioners working in the field. 
		  My research is being generously supported by <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2008461&HistoricalAwards=false">NSF</a>, <a href="https://sloan.org/fellowships/2025-Fellows">Sloan Foundation</a>, <a href="https://ai2050.schmidtsciences.org/fellows/">Schmidt Sciences</a>, <a href="https://research.google/outreach/">Google</a>, <a href="https://openai.com/">OpenAI</a>, <a href="https://www.amazon.science/research-awards/recipients/himabindu-lakkaraju-2020">Amazon</a>, <a href="https://www.jpmorgan.com/technology/artificial-intelligence/research-awards">JP Morgan</a>, <a href="https://research.adobe.com/data-science-research-awards/">Adobe</a>, <a href="https://www.bayer.com/en/">Bayer</a>, <a href="https://datascience.harvard.edu/">Harvard Data Science Initiative</a>, and <a href="https://d3.harvard.edu/">D^3 Insitute at Harvard</a>. 
		  My work has been featured in various major media outlets including the <a href="https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html">New York Times</a>, <a href="https://time.com/6289953/schumer-ai-regulation-explainability/">TIME magazine</a>, 
	<a href="https://fortune.com/2022/03/22/ai-explainable-radiology-medicine-crisis-eye-on-ai/">Fortune</a>, <a href="http://www.forbes.com/sites/timworstall/2013/09/02/how-to-craft-the-perfect-reddit-posting/">Forbes</a>, <a href="https://www.technologyreview.com/s/603763/how-to-upgrade-judges-with-machine-learning/">MIT Technology Review</a>, and <a href="https://hbr.org/2019/12/the-ai-transparency-paradox">Harvard Business Review</a>. 
		 </p>
		 <p> Please check out my <a href="./HimaCV.pdf">CV</a> for more details about me and my research. </p>
<p>
<font color="#cc0000">NOTE:</font> I am looking for motivated <font color="#cc0000">graduate and undergraduate students and postdocs</font> who are broadly interested in trustworthy machine learning and large pre-trained models. 
	If you are excited about this line of research and would like to work with me, please <a href="javascript:toggle_vis('contact')">read this</a> before contacting me. 
</p>		  <div id="contact" style="display:none"> 
		  <blockquote style=" padding: 10px; background-color: #EEEEEE;"><font size="3">
                  <p>Thank you for your interest in joining my lab! I am currently recruiting undergraduate, masters, and PhD students as well as postdoctoral fellows. </p>
			  <p>If you are interested in a <i>postdoc</i> position, please see <a href="https://himalakkaraju.github.io/openpositions.html">this page</a> for more details and application process. <!--send me an email with your 
 and a brief description of your research interests. Please use the subject line <font color="#cc0000">"Postdoctoral Fellowship Application"</font> in your email. --> </p> 
		  <p>If you are a <i>current or admitted undergraduate or masters or PhD student at Harvard</i>, please send me an email with your CV and a brief description of your research interests. Please use the subject line <font color="#cc0000">"Interested in Collaboration (Harvard Student)"</font> in your email. 
	  	  </p>
		  If you are <i>not (yet!) a student at Harvard and would like to pursue a PhD under my guidance</i>, <font color="#cc0000">please apply to BOTH the following PhD programs and mention my name in your statements and applications</font>: 
		  <ol>
			  <li> <a href="https://gsas.harvard.edu/programs-of-study/all/business-administration">PhD Program in Technology and Operations Management</a> at Harvard Business School. 
			  </li>
			  <li> <a href="https://www.seas.harvard.edu/computer-science/graduate-programs/how-apply">PhD Program in Computer Science</a> at Harvard SEAS.
			  </li>	  
		  </ol>
		  <p> I have few internship positions available for students who are already doing their PhD in the United States. If this is of interest, please send me an email with your CV and a brief description of your research interests. Please use the subject line <font color="#cc0000">"Internship Position (PhD Student)"</font> in your email. 
			  </p> 
		  <p>
		  Unfortunately, I will not be able to respond to any individual emails about admissions to masters or PhD programs. 
	 	  </p>
		  <!--<p> I am also open to research collaborations with students and faculty both within Harvard and elsewhere. If you are interested in collaborating with me and my group, please send me an email with a brief description of your research interests and background. Please do not forget to include a link to your webpage and/or your CV. </p>
	        -->  
		</blockquote>
	          </div>
		  </font>

	  
	</div>
	<font size="3">
      </div>

      <div class="page-header" id="news"><h3>Selected Achievements & News</h3></div>
      <div class="row">
        <div class="col-md-12">
          <ul>
		  <li><a href="https://sloan.org/fellowships/2025-Fellows">Alfred P. Sloan Fellow</a> in Computer Science, 2025</li>
		  <li><a href="https://nenlp.github.io/spr2025/accepted_work.html">Outstanding Paper Award</a> at New England NLP Symposium, 2025</li>
		  <li><a href="https://iisc.ac.in/events/iisc-distinguished-alumnus-alumna-awards-and-young-alumnus-alumna-medal-2024/">Distinguished Young Alumnus Award</a>, Indian Institute of Science, 2024</li>
		  <li><a href="https://ai2050.schmidtfutures.com/fellows/">AI2050 Early Career Fellowship</a> by Schmidt Sciences, 2024</li>
		  <li><a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2238714&HistoricalAwards=false">NSF CAREER Award</a>, 2023</li>
		  <li>Named <a href="https://www.nasonline.org/programs/kavli-frontiers-of-science/past-symposia/2023-uskfos.html">Kavli Fellow 2023</a> by National Academy of Sciences</li>
		  <li><a href="https://research.adobe.com/data-science-research-awards/">Adobe Data Science Research Award</a>, 2023</li>
		  <li> Outstanding Paper Award Honorable Mention, <a href="https://tsrml2022.github.io/">NeurIPS Workshop on Trustworthy and Socially Responsible Machine Learning</a>, 2022 </li>
		  <li> <a href="https://www.jpmorgan.com/technology/artificial-intelligence/research-awards">JP Morgan Faculty Research Award</a>, 2022 </li>
		  <li> Best Paper Award, <a href="https://icml.cc/virtual/2021/workshop/8358">ICML Workshop on Interpretable ML in Healthcare</a>, 2022 </li>
		  <li> <a href="https://www.amazon.science/research-awards/recipients/himabindu-lakkaraju-2020">Amazon Research Award</a>, 2021 </li>
		  <li> <a href="https://sites.google.com/view/aiforsocialgoodworkshop/2021-projects">Google AI for Social Good Research Award</a>, 2021 </li>
		  <li> Best Paper Runner Up, <a href="https://icml.cc/virtual/2021/workshop/8363">ICML Workshop on Algorithmic Recourse</a>, 2021 </li>
		  <li> <a href="https://research.google/outreach/research-scholar-program/recipients/">Google Research Award</a>, 2020 </li>
		  <li> <a href="https://prizes.fas.harvard.edu/hoopes-prize">Hoopes prize</a> for undergraduate thesis mentoring, Harvard University, 2020 </li>
		  <li> Named one of the world's <a href="https://www.innovatorsunder35.com/the-list/himabindu-lakkaraju/">35 innovators under 35 by MIT Tech Review</a>, 2019 </li>
		  <li> Named one of the world's <a href="https://www.vanityfair.com/news/2019/10/future-innovators-index-2019">top innovators to watch by Vanity Fair</a>, 2019 </li>
		  <li> Selected for the prestigious Cowles fellowship by Yale University, 2018 </li>
		  <li> INFORMS Best Data Mining Paper Award, 2017 </li>
		  <li> <a href="https://www.microsoft.com/en-us/research/blog/dissertation-grant-program-winners/">Microsoft Research Dissertation Grant</a>, 2017 </li>
		  <li> Named a <a href="http://risingstars.ece.cmu.edu/himabindu-lakkaraju/">Rising Star in Computer Science</a>, 2016 </li>
		  <li> <a href="https://buildyourfuture.withgoogle.com/programs"> Google Anita Borg Fellowship </a>, 2015 </li>
		  <li> <a href="https://vpge.stanford.edu/fellowships-funding/current-vpge-fellows/all-2013">Stanford Graduate Fellowship</a> (Awarded to Top 3% Students), 2013-17 </li>
		  <li> Eminence and Excellence Award, IBM Research, 2012 </li>
		  <li> Research Division Award, IBM Research, 2012 </li>
		  <li> Best Paper Award, <a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611972818.43">SIAM International Conference on Data Mining</a>, 2011 </li> 
      <br> 
      <li> My <a href="https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL">short course on Explainable AI</a> hosted by Stanford University is now available on Youtube (over 100,000 views) </li>
      <li>Excited to serve as the ethics co-chair for <a href="https://neurips.cc/">NeurIPS 2024 and 2025</a></li>
      <li>Excited to co-organize the workshop on <a href="https://regulatableml.github.io/">Regulatable ML: Bridging the Gaps between ML Research and Regulations</a> at NeurIPS 2023-2025.</li>
      <li>Excited to serve as the tutorial co-chair for WSDM 2024</li>
      <li>Excited to serve as the sponsorship co-chair for FAccT 2023</li>
      <li> Released the first version of <a href="https://open-xai.github.io/">OpenXAI</a>, a light-weight open source library to evaluate and benchmark post hoc explanation methods, 2022 </li>
      <li> Co-founded <a href="https://www.trustworthyml.org/">Trustworthy ML Initiative</a> to enable easy access to resources on trustworthy ML & to build a community of researchers/practitioners, 2020 </li>
      
      
      
          </ul>
        </div>
      </div>
 
      <div class="page-header" id="news"><h3>Upcoming and Recent Talks</h3></div>
      <div class="row">
        <div class="col-md-12">
          <ul>
		  <li><date>10.2025</date> Google DeepMind Agentic AI Safety Workshop </li>
		  <li><date>06.2025</date> Pinterest Distinguished Lecture </li>
		  <li><date>11.2024</date> <a href="https://blackboxnlp.github.io/2025/">EMNLP Workshop on BlackboxNLP</a> </li>
		  <li><date>10.2024</date> <a href="https://events.cornell.edu/event/lmss-cornell-tech-hima-lakkaraju-harvard">Learning Machines Seminar</a>, Cornell University </li>
		  <li><date>10.2024</date> <a href="https://rcais.github.io/">First Annual Summit on Responsible Computing, AI, and Society</a>, Georgia Tech</li>
		  <li><date>10.2024</date> <a href="https://sites.google.com/umd.edu/umd-alignment-workshop">AI Alignment Workshop</a>, University of Maryland</li>
		  <li><date>09.2024</date> Workshops on AI Regulation, India and Singapore</li>
		  <li><date>04.2024</date> <a href="https://nam.ai.princeton.edu/">Princeton University Workshop on Understanding in Natural and Artificial Minds</a></li>
		  <li><date>04.2024</date> <a href="https://www.cs.jhu.edu/event/cs-seminar-series-enforcing-right-to-explanation-algorithmic-challenges-and-opportunities/">Johns Hopkins CS Seminar Series</a></li>
		  <li> <date>03.2024</date><a href="https://www.sec.gov/"> US Securities and Exchange Commission</a></li>
		  <li><date>03.2024</date> Guest Lecture in Algorithmic Fairness Course, Cornell University</li>
		  <li><date>02.2024</date><a href="https://dsl.mit.edu/"> MIT Data Science Seminar Series</a></li>
		  <li><date>02.2024</date><a href="https://asset.seas.upenn.edu/event/himabindu-lakkaraju-harvard-university/"> UPenn Center for Safe, Explainable, and Trustworthy AI Seminar Series</a></li>
		  <li><date>02.2024</date><a href="https://ppai-workshop.github.io/"> AAAI Workshop on Privacy-Preserving Artificial Intelligence</a></li>
		  <li> <date>01.2024</date> NSF Workshop on Advanced Automated Systems, Contestability, and the Law </li>
		  <!--<li> <date>10.2023</date> <a href="https://fds.yale.edu/calendar_event/machine-learning-workshop/">Yale and Google Joint Workshop on Theory and Practice of Foundation Models</a></li>
		  <li> <date>10.2023</date> <a href="https://sites.google.com/view/genai-risks-workshop-oct-2023/">Google, Stanford, and UW Madison Workshop on Securing the Future of GenAI</a></li>
		  <li> <date>10.2023</date> <a href="https://meetings.informs.org/wordpress/phoenix2023/">INFORMS Annual Meeting</a>
		  <li> <date>07.2023</date> <a href="https://sites.google.com/view/imlh2023/home?authuser=1">ICML Workshop on Interpretable ML in Healthcare</a></li>
		  <li> <date>07.2023</date> <a href="https://sites.google.com/view/counterfactuals-icml/home">ICML Workshop on Counterfactuals in Minds and Machines</a></li>
		  <li> <date>07.2023</date> <a href="https://sites.google.com/view/rss2023-safe-autonomy">RSS Workshop on Safe Autonomy</a></li>
		  <li> <date>05.2023</date> <a href="https://rtml-iclr2023.github.io">ICLR Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models</a></li>
		  <li> <date>05.2023</date> <a href="https://sites.google.com/view/rainscmu">Responsible AI Workshop at Carnegie Mellon University </a></li>
		  <li> <date>04.2023</date> Guest Lecture at <a href="https://safeai-lab.github.io/taiat-spring2022.html">Carnegie Mellon University</a></li>
		  <li> <date>04.2023</date> <a href="https://mind-machine.ucsb.edu/meetings">Mind and Machine Intelligence Summit</a>, UC Santa Barbara</li>
		  <li> <date>04.2023</date> <a href="https://india.acm.org/education/learning/bootcamp-apr2023">ACM India Bootcamp on Responsible Computing</a>
		  <li> <date>03.2023</date> <a href="https://www.mlim-cornell.club">Cornell University and Weill Cornell Medicine </a></li>
		  <li> <date>03.2023</date> Guest Lecture at <a href="https://niloufar.org/human-centered-ai-course/"> UC Berkeley </a></li>
		  <li> <date>03.2023</date> <a href="https://www.nasonline.org/programs/kavli-frontiers-of-science/">Kavli Frontiers of Science Symposium </a></li>
		  <li> <date>03.2023</date> <a href="https://info.cohere.ai/talking-language-ai-5">Cohere AI</a></li>
		  <li> <date>02.2023</date> Keynote at <a href="https://r2hcai.github.io/AAAI-23/index.html"> AAAI Workshop on Representation Learning for Responsible Human-Centric AI </a> </li>
		  <li> <date>02.2023</date> <a href="https://sites.google.com/view/dai-2023"> AAAI Workshop on Deployable AI </a> </li>
		  <li> <date>12.2022</date> <a href="https://cyber.harvard.edu/">Berkman Klein Center, Harvard University</a> </li>
		  <li> <date>11.2022</date> Keynote at <a href="https://sites.google.com/view/wiml2022/">Women in Machine Learning (WiML) Workshop</a> Co-located with NeurIPS, 2022 </li>
		  <li> <date>11.2022</date> <a href="https://ml4health.github.io/2022/">Machine Learning for Health (ML4H) Workshop</a> Co-located with NeurIPS, 2022 </li>
		  <li> <date>11.2022</date> <a href="https://simons.berkeley.edu/workshops/schedule/16936">Simons Institute (Berkeley) Workshop on Societal Considerations and Applications</a></li>
		  <li> <date>11.2022</date> <a href="https://ide.mit.edu/events/fall-2022-ide-lunch-seminar-series/">MIT Initiative on the Digital Economy (IDE) Seminar Series</a></li>
		  <li> <date>11.2022</date> <a href="https://www.hdsiconference.org/">Harvard Data Science Initiative's Annual Conference</a>
		  <li> <date>10.2022</date> <a href="https://eccv22-arow.github.io/">ECCV Workshop on Adversarial Robustness in the Real World</a> </li>
		  <li> <date>08.2022</date> <a href="https://aisafety.stanford.edu/">Stanford Center for AI Safety</a> </li>
		  <li> <date>08.2022</date> <a href="https://www.amazon.science/tag/alexa">Amazon Alexa</a> Rising Star Speaker Series </li>
		  <li> <date>06.2022</date> <a href="https://xai4cv.github.io/workshop">CVPR Workshop on Explainable AI for Computer Vision</a> </li>
		  <li> <date>05.2022</date> <a href="https://pair2struct-workshop.github.io/">ICLR Workshop on Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data</a> </li>
		  <li> <date>05.2022</date> Keynote at <a href="https://aihealth.ischool.utexas.edu/AIHealthWWW2022/index.html">WWW Workshop on Explainable AI in Health</a> </li>
		  <li> <date>05.2022</date> Fiddler AI Fireside Chat </li>
		  <li> <date>04.2022</date> <a href="https://finreglab.org/artificial-intelligence-and-the-economy-charting-a-path-for-responsible-and-inclusive-ai-2">AI and the Economy</a> (U.S. Department of Commerce, National Institute of Standards and Technology, Stanford HAI, and the FinRegLab) </li>
		  <li> <date>04.2022</date> <a href="https://hai.stanford.edu/events/2022-hai-spring-conference-key-advances-artificial-intelligence">Stanford Human-Centered Artificial Intelligence (HAI) Conference</a> </li>
		  <li> <date>04.2022</date> <a href="https://digitaleconomy.stanford.edu/event/seminar-series-with-hima-lakkaraju/">Stanford Digital Econ Seminar</a> </li>
		  <li> <date>03.2022</date> University of Southern California </li>
		   <li> <date>12.2021</date> <a href="https://www.afciworkshop.org/">NeurIPS Workshop on Algorithmic Fairness through the Lens of Causality and Robustness</a> </li>
		  <li> <date>12.2021</date> <a href="https://xai4debugging.github.io/">NeurIPS Workshop on Explainable AI Approaches for Debugging and Diagnosis</a> </li>
		  <li> <date>12.2021</date> <a href="https://neurips.cc/virtual/2021/workshop/21835">NeurIPS Workshop on Human and Machine Decisions</a> </li>
		  <li> <date>12.2021</date> <a href="https://pinlabstechtalkdec21.splashthat.com/">Pinterest Tech Talks -- Distinguished Lecture</a> </li>
		  <li> <date>11.2021</date> Keynote at <a href="https://www.cikm2021.org/programme/keynote-speakers#towards-reliable-and-practicable-algorithmic-recourse">ACM Conference on Information and Knowledge Management</a> </li>
		  <li> <date>11.2021</date> <a href="https://www.nist.gov/news-events/events/2021/10/kicking-nist-ai-risk-management-framework"> NIST AI Risk Management Framework Workshop </a> </li>
		  <li> <date>11.2021</date> <a href="https://get.mccombs.utexas.edu/2021-catt-global-analytics-summit/#lp-pom-block-1164?utm_source=rss&utm_medium=rss&utm_campaign=catt-2021-global-analytics-summit-on-explainable-ai">Global Analytics Summit, University of Texas at Austin</a> </li>
		  <li>
			  <date>10.2021</date> <a href="https://www.fiddler.ai/fiddlers-explainable-ai-summit"> Explainable AI Summit, Fiddler.ai </a>
		  </li> 
		  <li>
			  <date>08.2021</date> <a href="https://www.youtube.com/watch?v=8Ym4oYTd8Fo"> Podcast on Explainability and Fairness in AI with Jay Shah </a>
		  </li> 
		  <li>
			  <date>08.2021</date> Keynote at <a href="https://sites.google.com/view/kdd-mlf-2020/">KDD Workshop on ML for Finance</a> 
		  </li>
		  <li> <date>07.2021</date> <a href="https://aiforgood.itu.int/event/trustworthy-ai-himabindu-lakkaraju/">AI for Good Summit organized by International Telecommunications Union & the United Nations </a> </li>
		  <li> 
			  <date>07.2021</date> <a href="https://sites.google.com/view/imlh2021/">ICML Workshop on Interpretable ML in Healthcare</a> 
		  </li>
		  <li>
			  <date>07.2021</date> <a href="http://www.neurosymbolic.org/events.html"> Neurosym Webinar Series, Jointly Organized by UPenn, MIT, Caltech, and Stanford </a>
		   <li> 
			  <date>06.2021</date> <a href="http://cvpr2021.thecvf.com/">CVPR Workshop on Responsible Computer Vision</a> 
		  </li>
		  <li>
			  <date>05.2021</date> Keynote at <a href="https://sites.google.com/view/rai-workshop/">ICLR Workshop on Responsible AI</a>
		  </li>
		  <li> 
			  <date>05.2021</date> <a href="https://www.cl.cam.ac.uk/research/ai/meetings/">University of Cambridge</a> 
		  </li>
		  <li>
			  <date>05.2021</date> Guest Lecture at <a href="https://exploreintrosems.stanford.edu/frosh/counterfactuals-science-what-ifs">Stanford University</a>
		  </li>
		  <li>
			  <date>04.2021</date> Invited Tutorial at <a href="https://www.chilconference.org/calendar.html#tab-tutorials">CHIL conference</a>
		  </li>
		  <li>
			  <date>04.2021</date> <a href="https://rss2workshop.github.io/">ASPLOS Workshop on Systems Architecture for Robust, Safe, and Resilient Software</a>
		  </li> 
		  <li>
			  <date>04.2021</date> <a href="https://personal-workshop.com/personal-mlsys-2021/">MLSys Workshop on Personalized Recommender Systems and Algorithms</a>
		  </li> 
		  <li>
			  <date>04.2021</date> <a href="https://www.sri.com/">SRI International</a>
		  </li>
		  <li>
			  <date>04.2021</date> Guest Lecture at <a href="https://projects.iq.harvard.edu/cs288/schedule">AI for Social Impact</a> course at Harvard University
		  </li>
		  <li> 
			  <date>02.2021</date> <a href="https://groups.cs.umass.edu/voicesofds/">Voices of Data Science, UMass Amherst</a> 
		  </li>
		  <li> 
			  <date>01.2021</date> <a href="https://www.cis.mpg.de/events/">Max Planck Symposium on Computing and Society</a> 
		  </li>
          <li>
		  <date>12.2020</date> Guest Lectures in <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa20/index.html">Advanced Machine Learning</a> and <a href="https://haiicmu.github.io/calendar/">Human-AI Interaction</a> Courses at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>
		  
	  </li> -->
          </ul>
        </div>
      </div>
      </font>

      <div class="page-header" id="news"><h3>Research</h3></div>
       <!-- ===== Publications (DMelis-style filter tabs) ===== -->
  <section class="pubs-wrap" style="max-width: 1200px; margin: 0 auto; padding: 5px 5px;">
  <p style="margin-top:0;color:#666">Browse by Topic or Publication Type.</p>
  <!-- Filter bar (tabs) -->
  <nav class="pub-filters" aria-label="Publication filters" role="tablist">
    <button class="pf-btn is-active" role="tab" aria-selected="true" data-filter="venue:conference,journal">All Publications</button>
    <button class="pf-btn" role="tab" data-filter="venue:conference">Conferences</button>
    <button class="pf-btn" role="tab" data-filter="venue:journal">Journals</button>
    <button class="pf-btn" role="tab" data-filter="venue:preprint">Selected Preprints</button>
    <button class="pf-btn" role="tab" data-filter="venue:patent">Patents</button>

    <!-- Force a second line for venue filters -->
    <span class="pf-break" aria-hidden="true"></span>
    
    <button class="pf-btn" role="tab" data-filter="topic:interpretability">Interpretability</button>
    <button class="pf-btn" role="tab" data-filter="topic:safety-robustness">Safety &amp; Reliability</button>
    <button class="pf-btn" role="tab" data-filter="topic:fairness">Fairness</button>
    <button class="pf-btn" role="tab" data-filter="topic:privacy-unlearning">Privacy &amp; Unlearning</button>
    <button class="pf-btn" role="tab" data-filter="topic:ai-decisionmaking">AI &amp; Decision Making</button>
    <button class="pf-btn" role="tab" data-filter="topic:humanai-collaboration">Human-AI Collaboration</button> 
    <button class="pf-btn" role="tab" data-filter="topic:ai-policy">AI &amp; Policy</button>
  </nav>

  <!-- One master list; filtering happens via JS -->
  <ul id="pub-list" class="pubs">
    <!-- Tag items with data-venue, data-topics, and data-selected (for highlighted preprints) -->
    <li class="pub"
        data-venue="preprint"
        <b>Who Gets Credit or Blame? Attributing Accountability in Modern AI Systems</b>
      <br>
      <span style="color:#555;">Shichang Zhang, Hongzhe Du, Karim Saraipour, Jiaqi Ma, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/pdf/2506.00175" aria-label="PDF: Who Gets Credit or Blame?"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations</b>
      <br>
      <span style="color:#555;">Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/pdf/2505.16004" aria-label="PDF: Interpretability Illusions"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>EvoLM: In Search of Lost Language Model Training Dynamics</b>
      <br>
      <span style="color:#555;">Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, Hanlin Zhang</span> <br>
      <a href="https://arxiv.org/pdf/2506.16029" aria-label="PDF: EvoLM"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Towards Unified Attribution in Explainable AI, Data-Centric AI, and Mechanistic Interpretability</b>
      <br>
      <span style="color:#555;">Shichang Zhang, Tessa Han, Usha Bhalla, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/pdf/2501.18887" aria-label="PDF: Unify Attribution"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Operationalizing the Blueprint for an AI Bill of Rights: Recommendations for Practitioners, Researchers, and Policy Makers</b>
      <br>
      <span style="color:#555;">Alex Oesterling, Usha Bhalla, Suresh Venkatasubramanian, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/abs/2407.08689" aria-label="PDF: Operationalizing"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Generalized Group Data Attribution</b>
      <br>
      <span style="color:#555;">Dan Ley, Suraj Srinivas, Shichang Zhang, Gili Rusak, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/pdf/2410.09940" aria-label="PDF: Generalized"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Generalizing Trust: Weak-to-Strong Trustworthiness in Language Models</b>
      <br>
      <span style="color:#555;">Martin Pawelczyk, Lillian Sun, Zhenting Qi, Aounon Kumar, Himabindu Lakkaraju</span> <br>
      <a href="https://arxiv.org/pdf/2501.00418" aria-label="PDF: Generalizing"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models</b>
      <br>
      <span style="color:#555;">Sree Harsha Tanneru, Dan Ley, Chirag Agarwal, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;"><font color="#cc0000">Research Highlight: </font><a href="https://openai.com/index/openai-o1-system-card/">OpenAI o1 System Card</a></span> <br>
      <a href="https://arxiv.org/abs/2406.10625" aria-label="PDF: Hardness CoT"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="preprint"
        <b>Manipulating Large Language Models to Increase Product and Content Visibility</b>
      <br>
      <span style="color:#555;">Aounon Kumar, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Featured in <a href="https://www.nytimes.com/2024/08/30/technology/ai-chatbot-chatgpt-manipulation.html">The New York Times</a> | <a href="https://www.theguardian.com/technology/2024/nov/03/the-chatbot-optimisation-game-can-we-trust-ai-web-searches">The Guardian</a> | <a href="https://cacm.acm.org/news/when-llms-learn-to-lie/">Communications of the ACM</a> | <a href="https://towardsdatascience.com/can-recommendations-from-llms-be-manipulated-to-enhance-a-products-visibility-64c64fa9cd24">Towards Data Science</a></span> <br>
      <a href="https://arxiv.org/abs/2406.10625" aria-label="PDF: Manipulating LLMs"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="ai-policy"
        <b>Advancing science- and evidence-based AI policy</b>
      <br>
      <span style="color:#555;">Rishi Bommasani, Sanjeev Arora, Jennifer Chayes, Yejin Choi, Mariano-Florentino Cuéllar, Li Fei-Fei, Daniel E. Ho, Dan Jurafsky, Sanmi Koyejo, Himabindu Lakkaraju, Arvind Narayanan, Alondra Nelson, Emma Pierson, Joelle Pineau, Scott Singer, Gaël Varoquaux, Suresh Venkatasubramanian, Ion Stoica, Percy Liang, and Dawn Song
</span> <br>
      <span style="color:#555;">Science, 2025. </span><br>
      <a href="https://www.science.org/doi/10.1126/science.adu8449" aria-label="PDF: Science"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="safety-robustness"
        <b>Detecting LLM-Generated Peer Reviews</b>
      <br>
      <span style="color:#555;">Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar B. Shah</span> <br>
      <span style="color:#555;">PLOS ONE, 2025. </span><br>
      <a href="https://arxiv.org/pdf/2503.15772" aria-label="PDF: PLOSONE"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence</b>
      <br>
      <span style="color:#555;">Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang</span> <br>
      <span style="color:#555;">Conference on Language Modeling (COLM), 2025.</span><br>
      <span style="color:#555;"><font color="#cc0000">Outstanding Paper Award</font>, New England NLP Symposium 2025</span><br>
      <a href="https://arxiv.org/abs/2504.02904" aria-label="PDF: COLM2025"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>On the Impact of Fine-Tuning on Chain-of-Thought Reasoning</b>
      <br>
      <span style="color:#555;">Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">The North American Chapter of the Association for Computational Linguistics (NAACL), 2025.</span><br>
      <a href="https://arxiv.org/pdf/2411.15382" aria-label="PDF: NAACL2025"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Quantifying Generalization Complexity for Large Language Models</b>
      <br>
      <span style="color:#555;">Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James Glass</span> <br>
      <span style="color:#555;">International Conference on Learning Representations (ICLR), 2025.</span><br>
      <a href="https://arxiv.org/pdf/2410.01769" aria-label="PDF: ICLR2025"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness, privacy-unlearning, fairness"
        <b>More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness</b>
      <br>
      <span style="color:#555;">Aaron Jiaxun Li, Satyapriya Krishna, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Learning Representations (ICLR), 2025.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 1.8%]</font></span><br>
      <a href="https://arxiv.org/pdf/2404.18870" aria-label="PDF: ICLR2025 RLHF Trust"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="privacy-unlearning"
        <b>Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</b>
      <br>
      <span style="color:#555;">Zhenting Qi, Hanlin Zhang, Eric P. Xing, Sham M. Kakade, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Learning Representations (ICLR), 2025.</span><br>
      <a href="https://arxiv.org/pdf/2402.17840" aria-label="PDF: ICLR2025 privacy"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)</b>
      <br>
      <span style="color:#555;">Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio Calmon, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2024.</span><br>
      <a href="https://arxiv.org/abs/2402.10376" aria-label="PDF: SpLiCE"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language models</b>
      <br>
      <span style="color:#555;">Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2403.03744" aria-label="PDF: MedSafetyBench"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="privacy-unlearning"
        <b>In-context Unlearning: Language Models as Few Shot Unlearners</b>
      <br>
      <span style="color:#555;">Martin Pawelczyk, Seth Neel, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2310.07579" aria-label="PDF: In-context Unlearning"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Understanding the Effects of Iterative Prompting on Truthfulness</b>
      <br>
      <span style="color:#555;">Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2402.06625" aria-label="PDF: Iterative Prompting"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Characterizing Data Point Vulnerability as Average-Case Robustness</b>
      <br>
      <span style="color:#555;">Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Uncertainty in Artificial Intelligence (UAI), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2307.13885" aria-label="PDF: UAI 2024"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>A Study on the Calibration of In-context Learning</b>
      <br>
      <span style="color:#555;">Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Himabindu Lakkaraju, Sham Kakade</span> <br>
      <span style="color:#555;">The North American Chapter of the Association for Computational Linguistics (NAACL), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2312.04021" aria-label="PDF: NAACL 2024"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="fairness"
        <b>Investigating the Fairness of Large Language Models for Predictions on Tabular Data</b>
      <br>
      <span style="color:#555;">Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">The North American Chapter of the Association for Computational Linguistics (NAACL), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2310.14607" aria-label="PDF: NAACL 2024 Fairness"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Quantifying Uncertainty in Natural Language Explanations of Large Language Models</b>
      <br>
      <span style="color:#555;">Sree Harsha Tanneru, Chirag Agarwal, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.</span><br>
      <span style="color:#555;"><font color="#cc0000">Spotlight Presentation</font>, NeurIPS Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models, 2023.</span><br>
      <a href="https://arxiv.org/pdf/2311.03533" aria-label="PDF: AISTATS 2024"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="fairness, privacy-unlearning"
        <b>Fair Machine Unlearning: Data Removal while Mitigating Disparities</b>
      <br>
      <span style="color:#555;">Alex Oesterling, Jiaqi Ma, Flavio Calmon, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2024.</span><br>
      <a href="https://arxiv.org/pdf/2307.14754" aria-label="PDF: AISTATS 2024 Fair Unlearning"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="interpretability, humanai-collaboration"
        <b>The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective</b>
      <br>
      <span style="color:#555;">Satyapriya Krishna*, Tessa Han*, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Transactions on Machine Learning Research (TMLR), 2024.</span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in</font> <a href="https://fortune.com/2022/03/22/ai-explainable-radiology-medicine-crisis-eye-on-ai/">Fortune Magazine</a></span><br>
      <a href="https://arxiv.org/pdf/2202.01602" aria-label="PDF: Disagreement"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Certifying LLM Safety Against Adversarial Prompting</b>
      <br>
      <span style="color:#555;">Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Li, Soheil Feizi, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Conference on Language Modeling (COLM), 2024.</span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in</font> <a href="
      https://www.sciencenews.org/article/generative-ai-chatbots-chatgpt-safety-concerns?trk=feed_main-feed-card_feed-article-content">Science News</a></span><br>
      <a href="https://arxiv.org/pdf/2309.02705v2" aria-label="PDF: Certified Defenses"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="interpretability, humanai-collaboration, ai-decisionmaking"
        <b>TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations</b>
      <br>
      <span style="color:#555;">Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju*, Sameer Singh*</span> <br>
      <span style="color:#555;">Nature Machine Intelligence, 2023.</span><br>
      <span style="color:#555;"><font color="#cc0000">Outstanding Paper Award Honorable Mention</font>, NeurIPS Workshop on Trustworthy and Socially Responsible ML, 2022.</span><br>
      <a href="https://www.nature.com/articles/s42256-023-00692-8.pdf" aria-label="PDF: TalkToModel"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="interpretability"
        <b>Evaluating Explainability for Graph Neural Networks</b>
      <br>
      <span style="color:#555;">Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, Marinka Zitnik</span> <br>
      <span style="color:#555;">Nature Scientific Data, 2023.</span><br>
      <a href="https://arxiv.org/pdf/2208.09339" aria-label="PDF: GNN Explainability"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Post Hoc Explanations of Language Models Can Improve Language Models</b>
      <br>
      <span style="color:#555;">Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2305.11426" aria-label="PDF: Post hoc LLM Explanations"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability</b>
      <br>
      <span style="color:#555;">Usha Bhalla*, Suraj Srinivas*, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2307.15007" aria-label="PDF: Verifiable Attributions"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Which Models have Perceptually-Aligned Gradients? An Explanation via Off-Manifold Robustness</b>
      <br>
      <span style="color:#555;">Suraj Srinivas*, Sebastian Bordt*, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2023.</span><br>
      <span style="color:#555;"><font color="#cc0000">Spotlight Presentation [Top 3%]</font></span><br>
      <a href="https://arxiv.org/pdf/2305.19101" aria-label="PDF: Perceptually-Aligned Gradients"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>M4: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities, and Models</b>
      <br>
      <span style="color:#555;">Xuhong Li, Mengnan Du, Jiamin Chen, Yekun Chai, Himabindu Lakkaraju, Haoyi Xiong</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2023.</span><br>
      <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/05957c194f4c77ac9d91e1374d2def6b-Paper-Datasets_and_Benchmarks.pdf" aria-label="PDF: M4 Benchmark"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>M4: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities, and Models</b>
      <br>
      <span style="color:#555;">Xuhong Li, Mengnan Du, Jiamin Chen, Yekun Chai, Himabindu Lakkaraju, Haoyi Xiong</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2023.</span><br>
      <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/05957c194f4c77ac9d91e1374d2def6b-Paper-Datasets_and_Benchmarks.pdf" aria-label="PDF: M4 Benchmark"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="ai-decisionmaking, humanai-collaboration"
        <b>When Does Uncertainty Matter?: Understanding the Impact of Predictive Uncertainty in ML Assisted Decision Making</b>
      <br>
      <span style="color:#555;">Sean McGrath, Parth Mehta, Alexandra Zytek, Isaac Lage, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Transactions on Machine Learning Research (TMLR), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2011.06167" aria-label="PDF: Predictive Uncertainty"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, privacy-unlearning, ai-policy"
        <b>Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten</b>
      <br>
      <span style="color:#555;">Satyapriya Krishna*, Jiaqi Ma*, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2302.04288" aria-label="PDF: RTE and RTBF"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, fairness"
        <b>On the Impact of Actionable Explanations on Social Segregation</b>
      <br>
      <span style="color:#555;">Ruijiang Gao, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2023.</span><br>
      <a href="https://proceedings.mlr.press/v202/gao23d/gao23d.pdf" aria-label="PDF: Explanations Segregation"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>On Minimizing the Impact of Dataset Shifts on Actionable Explanations</b>
      <br>
      <span style="color:#555;">Anna Meyer*, Dan Ley*, Suraj Srinivas, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Uncertainty in Artificial Intelligence (UAI), 2023.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 5%]</font></span><br>
      <a href="https://arxiv.org/pdf/2306.06716.pdf" aria-label="PDF: Dataset Shifts Explanations"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse</b>
      <br>
      <span style="color:#555;">Martin Pawelczyk, Teresa Datta, Johannes van-den-Heuvel, Gjergji Kasneci, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Learning Representations (ICLR), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2203.06768" aria-label="PDF: Recourse Trade-offs"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, privacy-unlearning"
        <b>On the Privacy Risks of Algorithmic Recourse</b>
      <br>
      <span style="color:#555;">Martin Pawelczyk, Himabindu Lakkaraju, Seth Neel</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2023.</span><br>
      <a href="https://arxiv.org/pdf/2211.05427" aria-label="PDF: Recourse Privacy"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations</b>
      <br>
      <span style="color:#555;">Tessa Han, Suraj Srinivas, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2022.</span><br>
      <span style="color:#555;"><font color="#cc0000">Best Paper Award</font>, ICML Workshop on Interpretable Machine Learning in Healthcare, 2022.</span><br>
      <a href="https://arxiv.org/pdf/2206.01254" aria-label="PDF: Function Approximation"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Flatten the Curve: Efficiently Training Low-Curvature Neural Networks</b>
      <br>
      <span style="color:#555;">Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, Francois Fleuret</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2206.07144" aria-label="PDF: Flatten"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>OpenXAI: Towards a Transparent Evaluation of Model Explanations</b>
      <br>
      <span style="color:#555;">Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2206.11104" aria-label="PDF: OpenXAI"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Data Poisoning Attacks on Off-Policy Evaluation Methods</b>
      <br>
      <span style="color:#555;">Elita Lobo, Harvineet Singh, Marek Petrik, Cynthia Rudin, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Uncertainty in Artificial Intelligence (UAI), 2022.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 5%]</font></span><br>
      
      <a href="https://openreview.net/pdf?id=BgbgH_Ls5lc" aria-label="PDF: data poisoning"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis</b>
      <br>
      <span style="color:#555;">Martin Pawelczyk, Chirag Agarwal, Shalmali Joshi, Sohini Upadhyay, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2106.09992" aria-label="PDF: Explanations Adversarial Examples"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods</b>
      <br>
      <span style="color:#555;">Chirag Agarwal, Marinka Zitnik, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2106.09078" aria-label="PDF: Probing GNN Explainers"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, fairness"
        <b>Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations</b>
      <br>
      <span style="color:#555;">Jessica Dai, Sohini Upadhyay, Ulrich Aivodji, Stephen Bach, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on AI, Society, and Ethics (AIES), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2205.07277" aria-label="PDF: Fairness Exp Quality"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, humanai-collaboration"
        <b>Towards Robust Off-Policy Evaluation via Human Inputs</b>
      <br>
      <span style="color:#555;">Harvineet Singh, Shalmali Joshi, Finale Doshi-Velez, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on AI, Society, and Ethics (AIES), 2022.</span><br>
      <a href="https://dl.acm.org/doi/10.1145/3514094.3534198" aria-label="PDF: off policy evals"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, humanai-collaboration"
        <b>A Human-Centric Take on Model Monitoring</b>
      <br>
      <span style="color:#555;">Murtuza N Shergadwala, Himabindu Lakkaraju, Krishnaram Kenthapadi</span> <br>
      <span style="color:#555;">AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2206.02868" aria-label="PDF: Model Monitoring"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>


    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, interpretability"
        <b>Towards Robust and Reliable Algorithmic Recourse</b>
      <br>
      <span style="color:#555;">Sohini Upadhyay*, Shalmali Joshi*, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2021.</span><br>
      <span style="color:#555;"><font color="#cc0000">Best Paper Runner Up</font>, ICML Workshop on Algorithmic Recourse, 2021.</span><br>
      <a href="https://arxiv.org/pdf/2102.13620" aria-label="PDF: Robust Recourse"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, interpretability"
        <b>Reliable Post hoc Explanations: Modeling Uncertainty in Explainability</b>
      <br>
      <span style="color:#555;">Dylan Slack, Sophie Hilgard, Sameer Singh, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2021.</span><br>
      <a href="https://arxiv.org/pdf/2008.05030" aria-label="PDF: Uncertainty Explanations"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, interpretability"
        <b>Counterfactual Explanations Can Be Manipulated</b>
      <br>
      <span style="color:#555;">Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, Sameer Singh</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2021.</span><br>
      <a href="https://arxiv.org/pdf/2106.02666" aria-label="PDF: Counterfactual Manipulated"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness, interpretability"
        <b>Learning Models for Algorithmic Recourse</b>
      <br>
      <span style="color:#555;">Alexis Ross, Himabindu Lakkaraju, Osbert Bastani</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2021.</span><br>
      <a href="https://arxiv.org/pdf/2011.06146" aria-label="PDF: Counterfactual Manipulated"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Towards the Unification and Robustness of Perturbation and Gradient Based Explanations</b>
      <br>
      <span style="color:#555;">Sushant Agarwal, Shahin Jabbari, Chirag Agarwal*, Sohini Upadhyay*, Steven Wu, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2021.</span><br>
      <span style="color:#555;"><font color="#cc0000">Spotlight Presentation</font></span><br>
      <span style="color:#555;">Shorter version presented at Foundations of Responsible Computing (FORC), 2022.</span><br>
      <a href="https://arxiv.org/pdf/2102.10618" aria-label="PDF: Unification Gradient Perturbation"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="fairness, safety-robustness"
        <b>Towards a Unified Framework for Fair and Stable Graph Representation Learning</b>
      <br>
      <span style="color:#555;">Chirag Agarwal, Himabindu Lakkaraju, Marinka Zitnik</span> <br>
      <span style="color:#555;">International Conference on Uncertainty in Artificial Intelligence (UAI), 2021.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 5%] </font></span><br>
      <a href="https://arxiv.org/pdf/2102.13186" aria-label="PDF: Fair Stable Graph"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="fairness, ai-decisionmaking"
        <b>Fair influence maximization: A welfare optimization approach</b>
      <br>
      <span style="color:#555;">Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos, Eric Rice, Milind Tambe</span> <br>
      <span style="color:#555;">AAAI International Conference on Artificial Intelligence (AAAI), 2021.</span><br>
      <a href="https://arxiv.org/pdf/2006.07906" aria-label="PDF: Fair influence"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="fairness, ai-decisionmaking, humanai-collaboration"
        <b>Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay of Human and Algorithmic Biases in Online Hiring</b>
      <br>
      <span style="color:#555;">Tom Suhr, Sophie Hilgard, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2021.</span><br>
      <a href="https://arxiv.org/pdf/2012.00423" aria-label="PDF: Fair Ranking"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, humanai-collaboration"
        <b>Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses</b>
      <br>
      <span style="color:#555;">Kaivalya Rawal and Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2020.</span><br>
      <a href="https://arxiv.org/abs/2009.07165" aria-label="PDF: Individualized Recourse"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Incorporating Interpretable Output Constraints in Bayesian Neural Networks</b>
      <br>
      <span style="color:#555;">Wanqian Yang, Lars Lorch, Moritz Gaule, Himabindu Lakkaraju, Finale Doshi-Velez</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NeurIPS), 2020.</span><br>
      <span style="color:#555;"><font color="#cc0000">Spotlight Presentation [Top 3%] </font></span><br>
      <a href="https://arxiv.org/pdf/2010.10969" aria-label="PDF: BNNs"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Robust and Stable Black Box Explanations</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Nino Arsov, Osbert Bastani</span> <br>
      <span style="color:#555;">International Conference on Machine Learning (ICML), 2020.</span><br>
      <a href="https://proceedings.icml.cc/static/paper_files/icml/2020/5945-Paper.pdf" aria-label="PDF: Robust Stable Exps"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness"
        <b>Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods</b>
      <br>
      <span style="color:#555;">Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2020.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 16.6%] </font></span><br>
      <span style="color:#555;"><font color="#cc0000">Best Paper (Non-Archival)</font>, AAAI Workshop on Safe AI, 2020</span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in</font> <a href="https://blog.deeplearning.ai/blog/the-batch-sony-goes-ai-intels-gpu-killers-transformer-networks-in-disguise-malicious-models-fool-bias-detection">deeplearning.ai</a> | <a href="https://hbr.org/2019/12/the-ai-transparency-paradox">Harvard Business Review</a></span><br>
      <a href="https://arxiv.org/pdf/1911.02508" aria-label="PDF: Fooling LIME and SHAP"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, safety-robustness, humanai-collaboration"
        <b>"How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Osbert Bastani</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2020.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 16.6%] </font></span><br>
      <a href="https://arxiv.org/pdf/1911.06473" aria-label="PDF: Fooling"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Faithful and Customizable Explanations of Black Box Models</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Ece Kamar, Rich Carauna, Jure Leskovec</span> <br>
      <span style="color:#555;">AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES), 2019.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 10%]</font></span><br>
      <a href="https://himalakkaraju.github.io/customizable.pdf" aria-label="PDF: Faithful Customizable"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="fairness, ai-decisionmaking"
        <b>Human Decisions and Machine Predictions</b>
      <br>
      <span style="color:#555;">Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan</span> <br>
      <span style="color:#555;">Quarterly Journal of Economics (QJE), 2018.</span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in</font> <a href="https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html">The New York Times</a>, <a href="https://www.technologyreview.com/2017/03/06/5361/how-to-upgrade-judges-with-machine-learning/">MIT Technology Review</a>, and <a href="https://hbr.org/2016/12/a-guide-to-solving-social-problems-with-machine-learning">Harvard Business Review</a></span><br>
      <a href="https://academic.oup.com/qje/article/doi/10.1093/qje/qjx032/4095198/Human-Decisions-and-Machine-Predictions#" aria-label="PDF: QJE"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan</span> <br>
      <span style="color:#555;">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2017.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 8.5%]</font></span><br>
      <a href="http://cs.stanford.edu/~jure/pubs/contraction-kdd17.pdf" aria-label="PDF: Selective Labels"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking, interpretability"
        <b>Learning Cost-Effective and Interpretable Treatment Regimes</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Cynthia Rudin</span> <br>
      <span style="color:#555;">International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.</span><br>
      <span style="color:#555;"><font color="#cc0000">INFORMS Best Data Mining Paper Award</font></span><br>
      <a href="https://arxiv.org/abs/1610.06972" aria-label="PDF: Cost-Effective"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="safety-robustness"
        <b>Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Eric Horvitz</span> <br>
      <span style="color:#555;">AAAI Conference on Artificial Intelligence (AAAI), 2017.</span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in </font><a href="https://www.bloomberg.com/news/articles/2017-12-04/researchers-combat-gender-and-racial-bias-in-artificial-intelligence">Bloomberg Technology</a></span><br>
      <a href="https://arxiv.org/abs/1610.09064" aria-label="PDF: Cost-Effective"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, ai-decisionmaking"
        <b>Confusions over Time: An Interpretable Bayesian Model to Characterize Trends in Decision Making</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Jure Leskovec</span> <br>
      <span style="color:#555;">Advances in Neural Information Processing Systems (NIPS), 2016.</span><br>
      <a href="https://snap.stanford.edu/hima/paper-temporal-confusions.pdf" aria-label="PDF: Confusions"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability, ai-decisionmaking"
        <b>Interpretable Decision Sets: A Joint Framework for Description and Prediction</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Stephen H. Bach, Jure Leskovec</span> <br>
      <span style="color:#555;">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2016.</span><br>
      <a href="https://cs.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf" aria-label="PDF: IDS"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="journal"
        data-topics="ai-decisionmaking"
        <b>Mining Big Data to Extract Patterns and Predict Real-Life Outcomes</b>
      <br>
      <span style="color:#555;">Michal Kosinki, Yilun Wang, Himabindu Lakkaraju, Jure Leskovec</span> <br>
      <span style="color:#555;">Psychological Methods, 2016.</span><br>
      <a href="http://mypersonality.org/wiki/lib/exe/fetch.php?media=psychological_methods.pdf" aria-label="PDF: Psych Methods"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>A Machine Learning Framework to Identify Students at Risk of Adverse Academic Outcomes</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Everaldo Aguiar, Carl Shan, David Miller, Nasir Bhanpuri, Rayid Ghani</span> <br>
      <span style="color:#555;">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2015.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 8.2%] </font></span><br>
      <a href="http://dssg.uchicago.edu/papers/montgomery_education.pdf" aria-label="PDF: DSSG 1"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>A Bayesian Framework for Modeling Human Evaluations</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Jure Leskovec, Jon Kleinberg, Sendhil Mullainathan</span> <br>
      <span style="color:#555;">SIAM International Conference on Data Mining (SDM), 2015.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 5%] </font></span><br>
      <a href="http://cs.stanford.edu/people/jure/pubs/evaluations-sdm15.pdf" aria-label="PDF: SDM Bayesian"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>Who, When, and Why: A Machine Learning Approach to Prioritizing Students at Risk of not Graduating High School on Time</b>
      <br>
      <span style="color:#555;">Everaldo Aguiar, Himabindu Lakkaraju, Nasir Bhanpuri, David Miller, Ben Yuhas, Kecia Addison, Rayid Ghani</span> <br>
      <span style="color:#555;">Learning Analytics and Knowledge Conference (LAK), 2015.</span><br>
      <a href="http://dl.acm.org/citation.cfm?id=2723619" aria-label="PDF: DSSG 2"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>What's in a name ? Understanding the Interplay Between Titles, Content, and Communities in Social Media</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Julian McAuley, Jure Leskovec</span> <br>
      <span style="color:#555;">International AAAI Conference on Weblogs and Social Media (ICWSM), 2013.</span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 3%]</font></span><br>
      <span style="color:#555;"><font color="#cc0000">Featured in</font> <a href="http://newsfeed.time.com/2013/09/03/how-to-succeed-on-reddit/">TIME</a>, <a href="http://www.forbes.com/sites/timworstall/2013/09/02/how-to-craft-the-perfect-reddit-posting/">Forbes</a>, <a href="http://phys.org/news/2013-09-stanford-trio-explore-success-formula.html">Phys.Org</a>, <a href="http://www.businessinsider.com/mathematicians-figured-out-how-to-execute-the-perfect-reddit-submission-2013-8">Business Insider</a>, <a href="https://www.newscientist.com/article/dn23867-what-reddit-likes-things-that-make-a-meme-explode/">New Scientist</a></span><br> 
      <a href="https://sites.google.com/site/himabindulv/papers/ICWSM2013.pdf?attredirects=0" aria-label="PDF: Interplay"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>


    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>Dynamic Multi-Relational Chinese Restaurant Process for Analyzing Influences on Users in Social Media</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Indrajit Bhattacharya, Chiranjib Bhattacharyya</span> <br>
      <span style="color:#555;">IEEE International Conference on Data Mining, 2012. </span><br>
      <span style="color:#555;"><font color="#cc0000">Oral Presentation [Top 8.6%]</font></span><br>
      <a href="http://arxiv.org/abs/1205.1456" aria-label="PDF: CRP"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>TEM: a novel perspective to modeling content on microblogs</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Hyung-Il Ahn</span> <br>
      <span style="color:#555;">International World Wide Web Conference (WWW), 2012. </span><br>
      <a href="http://www2012.wwwconference.org/proceedings/companion/p553.pdf" aria-label="PDF: TEM"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="interpretability"
        <b>Exploiting Coherence for the Simultaneous Discovery of Latent Facets and associated Sentiments</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indrajit Bhattacharya, Srujana Merugu</span> <br>
      <span style="color:#555;">SIAM International Conference on Data Mining (SDM), 2011. </span><br>
      <span style="color:#555;"><font color="#cc0000">Best Paper Award</font></span><br> 
      <a href="https://sites.google.com/site/himabindulv/papers/324.pdf?attredirects=0" aria-label="PDF: TEM"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>Attention prediction on social media brand pages</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Jitendra Ajmera</span> <br>
      <span style="color:#555;">ACM Conference on Information and Knowledge Management (CIKM), 2011. </span><br>
      <a href="https://sites.google.com/site/himabindulv/papers/cikm0226-lakkaraju.pdf?attredirects=0" aria-label="PDF: Attention Prediction"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="conference"
        data-topics="ai-decisionmaking"
        <b>Smart news feeds for social networks using scalable joint latent factor models</b>
      <br>
      <span style="color:#555;">Himabindu Lakkaraju, Angshu Rai, Srujana Merugu</span> <br>
      <span style="color:#555;">International World Wide Web Conference (WWW), 2011. </span><br>
      <a href="https://sites.google.com/site/himabindulv/papers/smartnewsfeeds1.pdf?attredirects=0" aria-label="PDF: smart news"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">PDF</a>
    </li>

    <li class="pub"
        data-venue="patent"
        <b>Extraction and grouping of feature words</b>
      <br>
      <span style="color:#555;">Chiranjib Bhattacharyya, Himabindu Lakkaraju, Kaushik Nath, Sunil Arvindam</span> <br>
      <a href="https://patents.google.com/patent/US8484228B2/" aria-label="PDF: patent 1"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">US8484228 B2</a>
    </li>

    <li class="pub"
        data-venue="patent"
        <b>Enhancing knowledge bases using rich social media</b>
      <br>
      <span style="color:#555;">Jitendra Ajmera, Shantanu Ravindra Godbole, Himabindu Lakkaraju, Bernard Andrew Roden, Ashish Verma</span> <br>
      <a href="https://patents.google.com/patent/US10192458B2/" aria-label="PDF: patent 1"
      style="margin-left:0px; border:1px solid #cbd5e1; border-radius:4px; font-size:.9em; text-decoration:none; display:inline-block;">US10192458 B2</a>
    </li>


    <!-- …add the rest; keep using data-venue / data-topics / data-selected -->
  </ul>
</section>

<style>
  .pub-filters{
    display:flex; flex-wrap:wrap; gap:8px 12px;
    border-bottom:1px solid #e5e7eb; padding-bottom:8px; width:100%;
  }
  .pf-break{ flex-basis:100%; height:0; padding:0; margin:0; }
  .pf-btn{
    appearance:none; border:0; background:none; cursor:pointer;
    padding:10px 12px; font-weight:600; white-space:nowrap;
    border-bottom:2px solid transparent; border-radius:8px 8px 0 0;
  }
  .pf-btn.is-active{ border-color:#111; }
  .pubs{ list-style:none; padding-left:0; display:grid; gap:14px; margin-top:14px; }
  .pub{ padding:12px 14px; border:1px solid #eee; border-radius:12px; }
  @media (max-width: 720px){
    .pf-btn{ padding:8px 10px; font-size:0.95rem; }
    .pf-break{ flex-basis:auto; } /* On small screens let them wrap naturally */
  }
  @media (prefers-reduced-motion:no-preference){
    .pub{ transition:opacity .15s ease, transform .15s ease; }
    .pub.is-hiding{ opacity:.0; transform:translateY(-2px); }
  }
</style>

<script>
(function(){
  const filterBar = document.querySelector('.pub-filters');
  const buttons   = [...filterBar.querySelectorAll('.pf-btn')];
  const list      = document.getElementById('pub-list');
  const items     = [...list.querySelectorAll('.pub')];

  const norm = s => (s||'').trim().toLowerCase();

  function matches(item, token){
    if (token === '*') return true;

    // topic:foo,bar  OR  venue:conference  OR  selected-preprints
    if (token.startsWith('topic:')){
      const want = token.replace('topic:','').split(',').map(norm);
      const have = (item.getAttribute('data-topics')||'')
                    .split(',').map(norm).filter(Boolean);
      return want.some(w => have.includes(w));
    }
    if (token.startsWith('venue:')){
      const wants = token.slice(6).split(',').map(s => s.trim().toLowerCase());
      const have  = (item.getAttribute('data-venue') || '').toLowerCase();
      return wants.includes(have);
    }
    if (token === 'selected-preprints'){
      return norm(item.getAttribute('data-venue')) === 'preprint'
             && item.getAttribute('data-selected') === 'true';
    }
    return false;
  }

  function applyFilter(token){
    items.forEach(li => {
      const show = matches(li, token);
      if (!show) { li.classList.add('is-hiding'); li.hidden = true; }
      else       { li.hidden = false; requestAnimationFrame(()=>li.classList.remove('is-hiding')); }
    });
  }

  filterBar.addEventListener('click', (e) => {
    const btn = e.target.closest('.pf-btn'); if (!btn) return;
    buttons.forEach(b => b.classList.remove('is-active'));
    btn.classList.add('is-active');
    buttons.forEach(b => b.setAttribute('aria-selected', b === btn ? 'true' : 'false'));
    applyFilter(btn.dataset.filter || '*');
  });

  // Optional: support ?tab=privacy or #tab=journal for deep links
  const urlTab = new URLSearchParams(location.search).get('tab')
              || (location.hash.startsWith('#tab=') ? location.hash.slice(5) : '');
  if (urlTab){
    const match = buttons.find(b => (b.dataset.filter||'').includes(urlTab.toLowerCase()) || b.textContent.toLowerCase()===urlTab.toLowerCase());
    if (match) match.click();
  }
})();

</script>

		
    <div class="page-header" id="students"> <h3>Advising</h3> </div>
		
    		<p>I am very fortunate to be working with the following core group of students, interns, postdocs, and research affiliates</p>
		<p>
		<ul>
			<li style="font-weight:normal">Shichang Zhang (Postdoc, Harvard University)</li>
			<li style="font-weight:normal">Aounon Kumar (Postdoc, Harvard University)</li>
			<li style="font-weight:normal">Martin Pawelczyk (Postdoc, Harvard University); Co-advised with Seth Neel</li>
			<li style="font-weight:normal">Usha Bhalla (PhD Student, Harvard University) </li>
			<li style="font-weight:normal">Dan Ley (PhD Student, Harvard University)</li>
			<li style="font-weight:normal">Alex Oesterling (PhD Student, Harvard University); Co-advised with Flavio Calmon </li>
			<li style="font-weight:normal">Paul Hamilton (PhD Student, Harvard University)</li>
			<li style="font-weight:normal">Zidi Xiong (PhD Student, Harvard University)</li>
			<li style="font-weight:normal">Jenny Wang (PhD Student, Harvard University)</li>
			<li style="font-weight:normal">Aaron Li (Masters Student, Harvard University)</li>
			<li style="font-weight:normal">Zhenting Qi (Masters Student, Harvard University)</li>
			<li style="font-weight:normal">Yanchen Liu (Masters Student, Harvard University)</li>
			<br>
			
				
		</ul>
		</p>
	    	<p> <b>Alumni</b> (Past Advisees, Close Collaborators, and Visitors): </p>
		<p>
	    	<ul>
			<li style="font-weight:normal">Jiaqi Ma (Postdoc, Harvard University --> Assistant Professor, UIUC)</li>
			<li style="font-weight:normal">Chirag Agarwal (Postdoc, Harvard University --> Assistant Professor, University of Virginia)</li>
			<li style="font-weight:normal">Suraj Srinivas (Postdoc, Harvard University --> Research Scientist, Robert Bosch)</li>
			<li style="font-weight:normal">Dylan Slack (PhD Student, UC Irvine --> Research Scientist, Google DeepMind)</li>
			<li style="font-weight:normal">Satyapriya Krishna (PhD Student, Harvard University --> Research Scientist, Amazon)</li>
			<li style="font-weight:normal">Tessa Han (PhD Student, Harvard University --> Postdoc, Harvard Medical School)</li>
			<li style="font-weight:normal">Sree Harsha Tanneru (Masters Student, Harvard University --> Research Engineer, Google DeepMind)</li>
			<li style="font-weight:normal">Aditya Karan (Masters Student, Harvard University --> PhD Student, UIUC CS)</li>
			<li style="font-weight:normal">Kaivalya Rawal (Masters Student, Harvard University --> Research Fellow, Oxford University)</li>
			<li style="font-weight:normal">Alexis Ross (Undergraduate Student, Harvard University -- Winner of Hoopes Prize for Best Undergrad Thesis --> PhD Student, MIT EECS)</li>
			<li style="font-weight:normal">Isha Puri (Undergraduate Student, Harvard University --> PhD Student, MIT EECS) </li>
			<li style="font-weight:normal">Jessica Dai (Undergraduate Student, Brown University --> PhD Student, UC Berkeley EECS) </li>	
			<li style="font-weight:normal">Eshika Saxena (Undergraduate Student, Harvard University --> AI Research Engineer, Meta) </li>
			<li style="font-weight:normal">Ethan Kim (Undergraduate Student, Harvard University --> Founding Engineer, VectorShift) </li>
			<li style="font-weight:normal">Catherine Huang (Undergrad, Harvard University --> Quant Trader, IMC Trading)</li>
			<li style="font-weight:normal">Charu Badrinath (Undergrad, Harvard University --> Engineer, Palantir Technologies)</li>
			<li style="font-weight:normal">Christina Xiao (Undergrad, Harvard University --> Engineer, Bloomberg)</li>
			<br>
			<li style="font-weight:normal">Sophie Hilgard (PhD Student, Harvard University --> Research Scientist, Twitter) </li>
			<li style="font-weight:normal">Sushant Agarwal (Masters Student, University of Waterloo --> PhD Student, Northeastern University) </li>
			<br>
			<li style="font-weight:normal">Harvineet Singh (PhD Student, New York University; Research Intern, Harvard University --> Postdoc UCSF/UC Berkeley) </li>
			<li style="font-weight:normal">Umang Bhatt (PhD Student, University of Cambridge; Research Intern, Harvard University --> Faculty Fellow, New York University) </li>
			<li style="font-weight:normal">Ruijiang Gao (PhD Student, University of Texas at Austin; Research Intern, Harvard University --> Assistant Professor, UT Dallas)</li>
			<li style="font-weight:normal">Elita Lobo (PhD Student, UMass Amherst; Research Intern, Harvard University) </li>
			<li style="font-weight:normal">Anna Meyer (PhD Student, University of Wisconsin; Research Intern, Harvard University)</li>
			<li style="font-weight:normal">Vishwali Mhasawade (PhD Student, New York University; Research Intern, Harvard University)</li>
			<li style="font-weight:normal">Nick Kroeger (PhD Student, University of Florida; Research Intern, Harvard University) </li>
			<li style="font-weight:normal">Chhavi Yadav (PhD Student, UC San Diego; Research Intern, Harvard University)</li>
			<li style="font-weight:normal">Tom Suhr (MS Student, TU Berlin; Research Fellow, Harvard University --> PhD Student, Max Planck Institute) </li>
			<li style="font-weight:normal">Davor Ljubenkov (Fullbright Scholar; Research Fellow, Harvard University)</li>
			
			
		</ul>
		</p>
    <div class="page-header" id="teaching"><h3>Teaching</h3></div>
     <font size="3">
	     
      <ul>
	<li><p> <a href="https://www.hbs.edu/mba/academic-experience/curriculum/Pages/required-curriculum.aspx">Introduction to Data Science and Machine Learning</a><br/>
	        Instructor <br />
		<i>Harvard University</i>, Fall 2020 - 2023.<br />
	</p></li>
	<li><p> <a href="https://interpretable-ml-class.github.io/">Explainable AI: From Simple Predictors to Complex Generative Models</a><br/>
	        Instructor <br />
		<i>Harvard University</i>, Fall 2019, Spring 2021, Spring 2023.<br />
	</p></li>
	<li><p>Introduction to Data Science<br />
	    Guest Lecture<br />
	    <i>Stanford Law School</i>, 2016.<br />	 
	</p></li>
	<li><p>Probability with Mathemagic<br />
	    Co-Instructor<br />
	    <i>Stanford Splash Initiative for High School Students</i>, 2016.<br />	 
	</p></li>
	<li><p>Mining Massive Datasets Course<br />
	    Teaching Assistant<br />
	    <i>Stanford Computer Science</i>, 2016.<br />	 
	</p></li>
	<li><p>Submodular Optimization<br />
	    Guest Lecture<br />
	    <i>Mining Massive Datasets Course, Stanford</i>, 2016.<br />	 
	</p></li>
	<li><p>Social and Information Network Analysis Course<br />
	    Head Teaching Assistant<br />
	    <i>Stanford Computer Science</i>, 2014.<br />	 
	</p></li>
	<li><p>Machine Learning Course<br />
	    Teaching Assistant<br />
	    <i>Indian Institute of Science</i>, 2010.<br />	 
	</p></li>
	<li><p>English and Mathematics<br />
	    Tutor<br />
	    <i>UNICEF's Teach India Initiative</i>, 2008 - 2010.<br />	 
	</p></li>
	</ul>
	</font>
    </div> <!-- /container -->


    <!-- Bootstrap core JavaScript
	 ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
      <script src="js/bootstrap.min.js"></script>
      <script src="js/docs.min.js"></script>
      <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
      <script src="js/ie10-viewport-bug-workaround.js"></script>

  <script>
  document.addEventListener('DOMContentLoaded', function () {
  const filterBar = document.querySelector('.pub-filters');
  const buttons   = Array.from(filterBar?.querySelectorAll('.pf-btn, .tab-btn') || []);
  const list      = document.getElementById('pub-list');
  const items     = Array.from(list?.querySelectorAll('.pub') || []);
  if (!filterBar || !buttons.length || !items.length) return;

  const norm = s => (s || '').trim().toLowerCase();

  function matches(item, token){
    if (!token || token === '*') return true;

    if (token === 'published') {
      const v = norm(item.getAttribute('data-venue'));
      return v === 'conference' || v === 'journal';
    }

    if (token === 'selected-preprints') {
      return norm(item.getAttribute('data-venue')) === 'preprint'
             && item.getAttribute('data-selected') === 'true';
    }

    if (token.startsWith('venue:')) {
      const wants = token.slice(6).split(',').map(norm);
      const have  = norm(item.getAttribute('data-venue'));
      return wants.includes(have);
    }

    if (token.startsWith('topic:')) {
      const wants = token.slice(6).split(',').map(norm);
      const have  = (item.getAttribute('data-topics') || '')
                      .split(',').map(norm).filter(Boolean);
      return wants.some(w => have.includes(w));
    }
    return false;
  }

  function applyFilter(token){
    items.forEach(li => {
      const show = matches(li, token);
      li.hidden = !show;                     // single source of truth
      li.classList.toggle('is-hiding', !show);
    });
  }

  // 1) Prefer deep link (?tab= or #tab=)
  const qpToken = new URLSearchParams(location.search).get('tab')
               || new URLSearchParams(location.search).get('filter')
               || (location.hash.startsWith('#tab=') ? location.hash.slice(5) : '');

  // 2) Choose initial active button
  let active = buttons.find(b => b.matches('.is-active,[aria-selected="true"]')) || buttons[0];

  // If URL requested a specific filter, honor it
  if (qpToken) {
    const t = norm(qpToken);
    const hit = buttons.find(b => norm(b.dataset.filter) === t);
    if (hit) active = hit;
  }

  // 3) Set states and APPLY FILTER ON LOAD
  buttons.forEach(b => {
    const selected = b === active;
    b.classList.toggle('is-active', selected);
    b.setAttribute('aria-selected', selected ? 'true' : 'false');
  });
  applyFilter(active.dataset.filter || '*');

  // 4) Click handler
  filterBar.addEventListener('click', (e) => {
    const btn = e.target.closest('.pf-btn, .tab-btn');
    if (!btn) return;
    buttons.forEach(b => {
      const selected = b === btn;
      b.classList.toggle('is-active', selected);
      b.setAttribute('aria-selected', selected ? 'true' : 'false');
    });
    applyFilter(btn.dataset.filter || '*');
  });
});
</script>

  </body>
</html>
